## âš™ï¸ How LLMs Work
### ğŸ“– Training Phases
1. **Pretraining**: Model learns general language patterns from vast amounts of text.
2. **Fine-Tuning**: Adapts to specific tasks like translation, question answering, or summarization.
3. **Prompt Engineering**: Optimizing inputs to guide the model's responses effectively.

### ğŸ”„ Key Techniques
- **Transformer Architecture** (Self-Attention, Multi-Head Attention)
- **Tokenization** (Byte Pair Encoding, WordPiece)
- **Fine-Tuning & Adaptation** (Instruction Tuning, RLHF)
- **Embedding Representations** (Word2Vec, BERT, GPT-based models)
